<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Ever wondered what your employee performance score would be? Part 4: Preprocessing with Dropping</title>

  <meta name="description" content="My name is Dennis Nguyen-Do. This is my personal blog." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
  <link href="" rel="canonical" />

  <!-- Feed -->

  <link href="/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="/theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->


    <link href="/2019/10/sharpestminds-project-part-4.html" rel="canonical" />

        <meta name="description" content="In the 4th post of this series, we get into the first step of preprocessing our data which includes steps like feature selection and...">

        <meta name="author" content="Nguyen-Do, Dennis">

        <meta name="tags" content="sharpestminds-project">
        <meta name="tags" content="data science">
        <meta name="tags" content="projects">
        <meta name="tags" content="employee performance">
        <meta name="tags" content="dropping">
        <meta name="tags" content="preprocessing">

        <meta property="og:locale" content="" />
    <meta property="og:site_name" content="Denny-4/7 Data Science Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Denny-4/7 Data Science Blog" />
    <meta property="og:description" content="View the blog." />
    <meta property="og:url" content="" />
      <meta property="og:image" content="//assets/head_cover_general.jpg" />

  <meta property="og:type" content="article">
            <meta property="article:author" content="/author/nguyen-do-dennis">
  <meta property="og:url" content="/2019/10/sharpestminds-project-part-4.html">
  <meta property="og:title" content="Ever wondered what your employee performance score would be? Part 4: Preprocessing with Dropping">
  <meta property="article:published_time" content="2019-10-07 16:27:00-05:00">
            <meta property="og:description" content="In the 4th post of this series, we get into the first step of preprocessing our data which includes steps like feature selection and...">

            <meta property="og:image" content="//assets/head_cover_general.jpg">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@Dennis00481552">
        <meta name="twitter:title" content="Ever wondered what your employee performance score would be? Part 4: Preprocessing with Dropping">
        <meta name="twitter:url" content="/2019/10/sharpestminds-project-part-4.html">

            <meta name="twitter:image:src" content="//assets/head_cover_general.jpg">

            <meta name="twitter:description" content="In the 4th post of this series, we get into the first step of preprocessing our data which includes steps like feature selection and...">
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>

          <ul>
              <li role="presentation"><a href="/pages/about.html">About</a></li>
          <ul>
              <li role="presentation"><a href="/pages/contact.html">Contact</a></li>
              <li class="nav-archives active" role="presentation"><a href="/archives.html">Archives</a></li>
          </ul>

    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <!-- <a class="home-button" href="" title="Home"><i class="ic ic-arrow-left"></i> Home</a> -->
                <a class="home-button" href="/index.html" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Ever wondered what your employee performance score would be? Part 4: Preprocessing with Dropping</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="/author/nguyen-do-dennis">Nguyen-do, dennis</a>
            | <time datetime="Mon 07 October 2019">Mon 07 October 2019</time>
        </span>
        <!-- TODO : Modified check -->
        
            <div class="post-cover cover" style="background-image: url('/assets/head_cover_articles.jpg')">            
        
      </div>    
    </header>    

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <div class="section" id="sharpestminds-project-series-part-4-preprocessing-with-dropping">
<h2>SharpestMinds Project Series Part 4: Preprocessing with Dropping</h2>
<p>If you are new to this post and would like some context, I'd highly suggest you read through the previous posts of this project series, as this is the fourth post of this series:</p>
<ul class="simple">
<li><a class="reference external" href="/2019/09/sharpestminds-project-part-1.html">Part 1 - Introduction</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-2.html">Part 2 - Background, Loading, EDA</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-3.html">Part 3 - More Exploratory Data Analysis</a></li>
</ul>
<p>In the last post, we touched exploratory data analysis and some of the insights it provided for us in terms of which variables and data points we might want to exclude, due to their less-than-desired-influence on our predictive modeling later on in the chapter. In this post, we dip our toes into the first preprocessing step of our data science project pipeline and consider omitting features and values from our final dataset by dropping. So without further ado, let's get into it.</p>
<div class="section" id="dropping-outliers">
<h3>Dropping Outliers</h3>
<p>From our previous post, I posted a method to fetch the outlier, extreme target variable values that will have an influence on our modeling step. We can observe theme with this snippet:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="k">import</span> <span class="n">iqr</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="n">percentile</span>

<span class="k">def</span> <span class="nf">get_outliers_and_extremes</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">num_attribute</span><span class="p">):</span>

    <span class="n">IQR</span> <span class="o">=</span> <span class="n">iqr</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">num_attribute</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">75</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="s1">&#39;raw&#39;</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s1">&#39;propagate&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">q1</span> <span class="o">=</span> <span class="n">percentile</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">num_attribute</span><span class="p">],</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">overwrite_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">q3</span> <span class="o">=</span> <span class="n">percentile</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">num_attribute</span><span class="p">],</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">overwrite_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">outliers</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="n">num_attribute</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">q1</span> <span class="o">-</span> <span class="p">(</span><span class="n">IQR</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">)))</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">num_attribute</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">q3</span> <span class="o">+</span> <span class="p">(</span><span class="n">IQR</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">)))]</span>
    <span class="n">extremes</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="n">num_attribute</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">q1</span> <span class="o">-</span> <span class="p">(</span><span class="n">IQR</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">)))</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">num_attribute</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">q3</span> <span class="o">+</span> <span class="p">(</span><span class="n">IQR</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">)))]</span>

    <span class="k">return</span> <span class="n">outliers</span><span class="p">,</span> <span class="n">extremes</span>


<span class="n">outliers</span><span class="p">,</span> <span class="n">extremes</span> <span class="o">=</span> <span class="n">get_outliers_and_extremes</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;job_performance&#39;</span><span class="p">)</span>
</pre></div>
<p>If you examine the shape of the extremes and outliers, you will notice that they are in fact the same for this dataset. In any case, we will drop them from the dataset with this:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">outliers</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="dropping-correlated-variables">
<h3>Dropping Correlated Variables</h3>
<p>If you have been following along with the <a class="reference external" href="/2019/10/sharpestminds-project-part-3.html">previous post</a> on data visualization, you got to see the wonderful and powerful abilities that the <tt class="docutils literal"><span class="pre">pandas-profiling</span></tt> module (a Python module that was built to extend the capabilities of the Pandas library) brought to the table in terms of preliminary data analysis and data visualization. The dataset profiling library was able to generate an HTML document which detailed, extensively (and I mean, quite extensively) which can be accessed through <a class="reference external" href="https://sjhh-nguyen-d.github.io/dataframe_profiling_report.html">this link</a>.</p>
<p>Without having too do too much heavy lifting, we are able discover, courtesy of the mighty and powerful capabilities that the pandas-profiling API has to offer, the names of the features in our dataset which are highly correlated, which might dubiously influence the modeling process (in the way of introducing biases to our predictions of new employee examples.) We can identify them with this these few lines of code:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas_profiling</span>

<span class="n">profile</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">profile_report</span><span class="p">()</span>
<span class="n">rejected_variables</span> <span class="o">=</span> <span class="n">profile</span><span class="o">.</span><span class="n">get_rejected_variables</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;There were {len(rejected_variables} highly correlated variables that were detected&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rejected_variables</span><span class="p">)</span>
</pre></div>
<p>Output:</p>
<pre class="literal-block">
['earnhrbonus',
'earnhrbonusppp',
'earnmthall',
'earnmthallppp',
'earnmthbonus',
'earnmthbonusppp',
'earnmthppp',
'isco2l',
'v1',
'v100',
'v110',
'v145',
'v156',
'v160',
'v163',
'v169',
'v231',
'v235',
'v283',
'v41',
'v45',
'v52',
'v63',
'v81',
'v87',
'v97',
'yrsqual_t']
There were 36 highly correlated variables that were detected
</pre>
<p>As we can see, there were quite a lot of variables that were detected by the profiling engine as being very highly correlated. As part of the initial preprocessing and feature selection step, we can choose to drop these variables based on these suggestions from the engine or keep them for other options for preprocessing but we're just going to drop them based on the suggestions. We will do just this:</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">rejected_variables</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>Output: <tt class="docutils literal">(20000, 353)</tt></p>
</div>
<div class="section" id="id1">
<h3>Dropping Correlated Variables</h3>
<p>We now see that we've reduced the number of features that are in our dataframe to 353. But that's still a large number of features in our data set. How would we know if the quality of the data in the remaining data points is of a lesser quality (i.e., lacking)? And if so, how much of it can we omit from our dataset based on poor quality?</p>
<div class="section" id="dropping-data-column-wise">
<h4>Dropping Data Column-Wise</h4>
<p>One way we can approach this is to set a column-wise threshold fraction by which a column must meet in terms of completeness, in order to be considered elligible for retainment. Python uses <tt class="docutils literal">nan</tt> to indicate that a value is missing from the dataset, which is equivalent to numpy's <tt class="docutils literal">numpy.nan</tt> value. However, in the real world, sometimes the value that is used to indicate a missing value is something other than a blank space such as &quot;unavailable&quot;, &quot;NA&quot;, &quot;9999&quot;, or anything else. It is therefore important to reencode these in or dataset so that we can get a better estimate as to the true proportion of the data that is unavailable to us.</p>
<div class="highlight"><pre><span></span><span class="n">considered_missing_values</span> <span class="o">=</span> <span class="p">[</span>
<span class="s1">&#39;999&#39;</span><span class="p">,</span> <span class="mi">9995</span><span class="p">,</span> <span class="s1">&#39;9995&#39;</span><span class="p">,</span> <span class="mi">9996</span><span class="p">,</span>
<span class="s1">&#39;9996&#39;</span> <span class="p">,</span><span class="mi">9997</span><span class="p">,</span> <span class="s2">&quot;9997&quot;</span><span class="p">,</span> <span class="mi">9998</span><span class="p">,</span>
<span class="s1">&#39;9998&#39;</span><span class="p">,</span> <span class="mi">9999</span><span class="p">,</span> <span class="s1">&#39;9999&#39;</span><span class="p">,</span> <span class="s1">&#39;99999&#39;</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">to_replace</span><span class="o">=</span><span class="n">considered_missing_values</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
</pre></div>
<p>After the missing values have been encoded, we can use the following code to print out a series containing the features along with the proprotion of missing values, by feature:</p>
<div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;{((df.isnull().sum().sort_values(ascending=False)[df.isnull().sum() &gt; 1000]/df.shape[0]) * 100)[:10]}&quot;</span><span class="p">)</span>
</pre></div>
<p>Output:</p>
<pre class="literal-block">
v262    100.000
v44      99.985
v76      99.965
v144     99.960
v199     99.955
v159     99.925
v10      99.905
v172     99.885
v110     99.780
v160     99.775
dtype: float64%
</pre>
<p>As we can see from just from this pandas series of sorted missing value proportions - there are quite a large number of columns with missing values. What we can do is set a threshold of 60%, and any column that meets the threshold of having greater or equal to 60% of its values missing will be added to the list of features to be dropped.</p>
<p>..code-block:: python3</p>
<blockquote>
col_drop_threshold = 0.6
more_than_60_missing = [feature for feature in df.columns if (df[feature].isnull().sum() / df.shape[0]) &gt;= col_drop_threshold]
pp.pprint(sorted(more_than_60_missing)[:10])
print(f&quot;There are {len(more_than_60_missing)} features that have more than or equal to 60% of it's data missing.&quot;)</blockquote>
<p>Output:</p>
<pre class="literal-block">
[   'earnmthselfppp',
    'imyrs',
    'imyrs_c',
    'leaver1624',
    'v10',
    'v100',
    'v107',
    'v109',
    'v11',
    'v110']
There are 114 features that have more than or equal to 60% of it's data missing.
</pre>
<p>That is a considerable number of variables that have been dropped due to insufficient data. Dropping these variables would equate to dropping about a quarter of the number of features we originally started with, and do this we will.</p>
<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">more_than_60_missing</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>Output: <tt class="docutils literal">(15985, 239)</tt></p>
<p>This leaves us left with a little less than 2/3's of our original number of columns. Data points with an extensive proportion of their values can, in rare cases,provide us information as to the underlying nature of our data and how the data itself was collected. In most cases, however, having the majority of our data missing is a bad thing, and we want to do like Marie Kondo and because they don't spark any joy in our lives anymore.</p>
</div>
<div class="section" id="dropping-data-row-wise">
<h4>Dropping Data Row-Wise</h4>
<p>We can drop data row-wise by the same measure we chose to drop data column-wise, that is, using a threshold approach, and we can do that with these lines of code:</p>
<div class="highlight"><pre><span></span><span class="c1"># Data points with percentage of data missing and greater will be dropped from the dataset</span>
<span class="n">dropthreshold</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># get the data points in the dataframe that have &gt;= 30% of their data missing</span>
<span class="n">more_than_20_missing_rows</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">dropthreshold</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span>
    <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span> <span class="o">&lt;</span> <span class="n">dropthreshold</span>
<span class="p">)]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>Output: <tt class="docutils literal">(14566, 239)</tt></p>
</div>
<div class="section" id="dropping-redundant-information">
<h4>Dropping Redundant Information</h4>
<p>At this point, we've whittled down the proportion of missing values by quite a bit. Some features we can identify algorithmically, or by some criterion, as elligible for dropping. Sometimes, this cannot be done without having the coding scheme or domain knowledge of how you retrieved your data. Luckily, if you've been provided a code book or have consulted your local, registered dataset provider, you will have even further insight as to what can or cannot be dropped for redundancy. One example of such redundancy can be seen in different versions of coding for the same information (e.g., the 2007 encoding for a value vs. the 2018 encoding scheme of a value). This case, we used our domain knowledge we received from other experts in our department, and will rule out which columns can be dropped from this dataset. In a way this step is similar to the feature selection step, but we will choose to include it in this section.</p>
<p>Our strategy then is to simply hand pick the feature sets that represent the same information, select the one that has the most complete information and discard the remaining one.</p>
<div class="highlight"><pre><span></span><span class="n">redundant_features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;readytolearn_wle_ca&quot;</span><span class="p">,</span> <span class="s2">&quot;icthome_wle_ca&quot;</span><span class="p">,</span> <span class="s2">&quot;ictwork_wle_ca&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;influence_wle_ca&quot;</span><span class="p">,</span> <span class="s2">&quot;planning_wle_ca&quot;</span><span class="p">,</span> <span class="s2">&quot;readhome_wle_ca&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;readwork_wle_ca&quot;</span><span class="p">,</span> <span class="s2">&quot;taskdisc_wle_ca&quot;</span><span class="p">,</span> <span class="s2">&quot;writhome_wle_ca&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;writwork_wle_ca&quot;</span><span class="p">,</span> <span class="s2">&quot;ageg10lfs&quot;</span><span class="p">,</span> <span class="s2">&quot;ageg10lfs_t&quot;</span><span class="p">,</span> <span class="s2">&quot;edcat7&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;edcat8&quot;</span><span class="p">,</span> <span class="s2">&quot;isco2c&quot;</span><span class="p">,</span> <span class="s2">&quot;isic2c&quot;</span><span class="p">,</span> <span class="s2">&quot;earnflag&quot;</span><span class="p">,</span> <span class="s2">&quot;reg_tl2&quot;</span><span class="p">,</span> <span class="s2">&quot;lng_bq&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;lng_ci&quot;</span><span class="p">,</span> <span class="s2">&quot;edlevel3&quot;</span><span class="p">,</span> <span class="s2">&quot;nfehrsnjr&quot;</span><span class="p">,</span> <span class="s2">&quot;nfehrsjr&quot;</span><span class="p">,</span> <span class="s2">&quot;fnfe12jr&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;fnfaet12jr&quot;</span><span class="p">,</span> <span class="s2">&quot;faet12jr&quot;</span><span class="p">,</span> <span class="s2">&quot;faet12njr&quot;</span><span class="p">,</span> <span class="s2">&quot;fe12&quot;</span><span class="p">,</span> <span class="s2">&quot;monthlyincpr&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;earnhrdcl&quot;</span><span class="p">,</span> <span class="s2">&quot;earnhrbonusdcl&quot;</span><span class="p">,</span> <span class="s2">&quot;row&quot;</span><span class="p">,</span> <span class="s2">&quot;uni&quot;</span><span class="p">,</span> <span class="s2">&quot;cntryid_e&quot;</span><span class="p">,</span> <span class="s2">&quot;v205&quot;</span><span class="p">,</span> <span class="s2">&quot;v270&quot;</span>
                 <span class="p">]</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">redundant_features</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>Output: <tt class="docutils literal">(14424, 204)</tt></p>
<p>The naming convention of these variables also gives hiint as to what is encoded in the values of these variables such as different encoding schemes, a more granular measurement of information, or an ordinalized organization of a numeric variable. Each of these offer different levels of signal to our model, however, for simplicity sake, we remove the versions of the features that have the largest proportions of their data missing.</p>
</div>
<div class="section" id="conclusion">
<h4>Conclusion</h4>
<p>In summation, what remains of the data after the initial preprocessing step of dropping some variables for due to insufficient data, redudant and highly correlated features is a dataframe of shape <tt class="docutils literal">(14424, 204)</tt>. We covered a a few methods to identify such data as well as the dropping operation for row-wise and column-wise data from the Pandas library. In the <a class="reference external" href="/2019/10/sharpestminds-project-part-5.html">next post</a>, we will begin the next preprocessing step of our pipeline, in which we prepare our data for further processing of by manipulating and encoding our data so that we can perform operations on the data later down the road. Until then...ciao!</p>
<!-- todo:
things to do
conclusory paragraph about what the next step of the project isEver wondered what your employee performance score would be? Part-3 -->
</div>
</div>
</div>

            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Ever wondered what your employee performance score would be? Part 4: Preprocessing with Dropping&amp;url=/2019/10/sharpestminds-project-part-4.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/2019/10/sharpestminds-project-part-4.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=/2019/10/sharpestminds-project-part-4.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="/tag/sharpestminds-project">sharpestminds-project</a><a href="/tag/data-science">data science</a><a href="/tag/projects">projects</a><a href="/tag/employee-performance">employee performance</a><a href="/tag/dropping">dropping</a><a href="/tag/preprocessing">preprocessing</a>                </aside>

                <div class="clear"></div>

 

                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>
  
    <footer id="footer">
      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a></span>
          <span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
        </section>
      </div>
    </footer>
  </section>

  <script type="text/javascript" src="/theme/js/script.js"></script>
  
</body>
</html>
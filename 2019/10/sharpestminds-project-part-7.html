<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Ever wondered what your employee performance score would be? Part 7: Feature Selection</title>

  <meta name="description" content="My name is Dennis Nguyen-Do. This is my personal blog." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
  <link href="" rel="canonical" />

  <!-- Feed -->

  <link href="/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="/theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->


    <link href="/2019/10/sharpestminds-project-part-7.html" rel="canonical" />

        <meta name="description" content="In this brief seventh post of the SharpestMinds project series we will go through the feature selection for our dataset and figure out...">

        <meta name="author" content="Nguyen-Do, Dennis">

        <meta name="tags" content="sharpestminds-project">
        <meta name="tags" content="data science">
        <meta name="tags" content="projects">
        <meta name="tags" content="employee performance">
        <meta name="tags" content="preprocessing">
        <meta name="tags" content="feature selection">
        <meta name="tags" content="feature engineering">

        <meta property="og:locale" content="" />
    <meta property="og:site_name" content="Denny-4/7 Data Science Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Denny-4/7 Data Science Blog" />
    <meta property="og:description" content="View the blog." />
    <meta property="og:url" content="" />
      <meta property="og:image" content="//assets/head_cover_general.jpg" />

  <meta property="og:type" content="article">
            <meta property="article:author" content="/author/nguyen-do-dennis">
  <meta property="og:url" content="/2019/10/sharpestminds-project-part-7.html">
  <meta property="og:title" content="Ever wondered what your employee performance score would be? Part 7: Feature Selection">
  <meta property="article:published_time" content="2019-10-30 18:26:00-05:00">
            <meta property="og:description" content="In this brief seventh post of the SharpestMinds project series we will go through the feature selection for our dataset and figure out...">

            <meta property="og:image" content="//assets/head_cover_general.jpg">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@Dennis00481552">
        <meta name="twitter:title" content="Ever wondered what your employee performance score would be? Part 7: Feature Selection">
        <meta name="twitter:url" content="/2019/10/sharpestminds-project-part-7.html">

            <meta name="twitter:image:src" content="//assets/head_cover_general.jpg">

            <meta name="twitter:description" content="In this brief seventh post of the SharpestMinds project series we will go through the feature selection for our dataset and figure out...">
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>

          <ul>
              <li role="presentation"><a href="/pages/about.html">About</a></li>
          <ul>
              <li role="presentation"><a href="/pages/contact.html">Contact</a></li>
              <li class="nav-archives active" role="presentation"><a href="/archives.html">Archives</a></li>
          </ul>

    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <!-- <a class="home-button" href="" title="Home"><i class="ic ic-arrow-left"></i> Home</a> -->
                <a class="home-button" href="/index.html" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Ever wondered what your employee performance score would be? Part 7: Feature Selection</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="/author/nguyen-do-dennis">Nguyen-do, dennis</a>
            | <time datetime="Wed 30 October 2019">Wed 30 October 2019</time>
        </span>
        <!-- TODO : Modified check -->
        
            <div class="post-cover cover" style="background-image: url('/assets/head_cover_articles.jpg')">            
        
      </div>    
    </header>    

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <div class="section" id="sharpestminds-project-series-part-7-feature-selection">
<h2>SharpestMinds Project Series Part 7: Feature Selection</h2>
<p>If you are new to this post and would like some context, I'd highly suggest you read through the previous posts of this project series, as this is the seventh post of this series:</p>
<ul class="simple">
<li><a class="reference external" href="/2019/09/sharpestminds-project-part-1.html">Part 1 - Introduction</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-2.html">Part 2 - Background, Loading, EDA</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-3.html">Part 3 - More Exploratory Data Analysis</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-4.html">Part 4 - Dropping Data</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-5.html">Part 5 - Categorical Data Encoding</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-6.html">Part 6 - Imputing Missing Values</a></li>
</ul>
<p>In the previous post, we approached the problem of missing values with the use of very useful package in the PYPI library called <tt class="docutils literal">missingpy</tt> and used its <tt class="docutils literal">KNNImputer</tt> class to help better impute for the missings values in our dataset. After the imputation of missing values, which were preivously encoded with numeric values, we had to perform rounding for the categorical discrete values of our dataset to make sure that . This is the seventh post of my SharpestMinds project series go into process of selecting the features to be included to be used for the modeling step of our pipeline. We make use some computationally intensive, but powerful tooling provided for us through the <tt class="docutils literal">mlxtend</tt> python library from PYPI as well as the <tt class="docutils literal">sklearn.feature_selection</tt> submodule. If you want to follow along with the full codebase in the Jupyter IPython Notebook, you can do so at the <a class="reference external" href="https://github.com/SJHH-Nguyen-D/sharpestminds-project">link to the repo</a>) at the <tt class="docutils literal">sharpestminds_project.ipynb</tt> file or the <tt class="docutils literal">main.py</tt> file. Let's get into it!</p>
<div class="section" id="feature-selection">
<h3>Feature Selection</h3>
<p>In the data science pipeline, the process of 'feature selection' refers to selecting a subset of the features or attributes of our dataset to include in the final modeling process. This step could be one of the most intensive steps alongside feature engineering, and is a means to produce the most signal to our model during the training and prediction step. The reason we perform feature selection is to remove to the chaff from the wheat - that is, to remove the useless features from the truly useful ones, which would not only improve the predictive ability of our model but also improve the training times for the training of our model. This process often requires in-depth technical domain knowledge (which can be very resource expensive) or an algorithmic search strategy by which we can approach feature selection. We will go over this latter part with the <tt class="docutils literal"><span class="pre">scikit-learn</span></tt> library's <tt class="docutils literal">feature_selection</tt> classes, which include meta-transformative estimators. In addition, we will go over the powerful and mighty 'automatic' approach to feature selection with the <tt class="docutils literal">mlxtend</tt> library. Each one of these approaches has their specific nuances for use and we will go over high level as to how they work as well as which one I ultimately chose to employ.</p>
<div class="section" id="univariate-feature-elimination">
<h4>Univariate Feature Elimination</h4>
<p>Using the univariate feature elimination strategy, we use univariate statistics (e.g., chi-squared test of independence for categorical data, f-test for regression, mutual information for regression) and specify a number of features to be included in the final dataset that have quantified a specified level of statistical significance (typically alpha=0.05 or alpha=0.01). The <tt class="docutils literal">sklearn</tt> library also provides the convenience of selecting the top user-specified percentile of features to be included. This algorithmic approach to feature selection is among the simplest, and the one I will employ for my project as it doesn't take long run.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">univariate_feature_selection_with_GUS</span><span class="p">(</span><span class="n">dataframe</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; uses univariate statistics such as mutual information regression to select the k best features</span>
<span class="sd">    for a regression problem &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">GenericUnivariateSelect</span><span class="p">,</span> <span class="n">mutual_info_regression</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">xy_split</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
    <span class="n">select_features_gus</span> <span class="o">=</span> <span class="n">GenericUnivariateSelect</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">mutual_info_regression</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;k_best&quot;</span><span class="p">,</span> <span class="n">param</span><span class="o">=</span><span class="nb">round</span><span class="p">((</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time for feature selection: </span><span class="si">{}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">GenericUnivariateSelect</span><span class="o">.</span><span class="n">scores_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">select_features_gus</span>

<span class="n">selected_features</span> <span class="o">=</span> <span class="n">univariate_feature_selection_with_GUS</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">dataframe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">selected_features</span><span class="p">)</span>
<span class="n">dataframe</span><span class="p">[</span><span class="s2">&quot;job_performance&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>
<p>Running a simple RandomForestRegressor out of the box gives us positive results, however, as a sidenote, there is very clearly heavy biases in these predictions.</p>
<div class="highlight"><pre><span></span><span class="n">dataframe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">selected_features</span><span class="p">)</span>
<span class="n">dataframe</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">62</span><span class="p">:</span><span class="s2">&quot;job_performance&quot;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">split_dataframe</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
<span class="n">rfr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;coefficient of determination R^2 of our estimator out of the box: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rfr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total training time: </span><span class="si">{}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">coefficient</span> <span class="n">of</span> <span class="n">determination</span> <span class="n">R</span><span class="o">^</span><span class="mi">2</span> <span class="n">of</span> <span class="n">our</span> <span class="n">estimator</span> <span class="n">out</span> <span class="n">of</span> <span class="n">the</span> <span class="n">box</span><span class="p">:</span> <span class="mf">0.9469250788745618</span>
<span class="n">Total</span> <span class="n">training</span> <span class="n">time</span><span class="p">:</span> <span class="mf">2.3305274920003285</span><span class="n">s</span>
</pre></div>
</div>
<div class="section" id="removing-features-with-low-variance">
<h4>Removing Features with Low Variance</h4>
<p>Numeric features that contain little to no variance do no provide more noise to signal ratio by being included in the modeling processes. Setting a cut-off value to this variance (i.e., the square of the standard deviation), allows us to eliminate noisy variables below this cut-off. By default, a variable containing only one value would be eliminated using this approach (cutoff=0.0). This is among the simplest feature selection algorithms and one of the least computationally expensive to employ.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">select_features_var_threshold</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Features with a training-set variance lower than this threshold will be removed.  &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">VarianceThreshold</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span>  <span class="o">=</span> <span class="n">xy_split</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
    <span class="n">var_threshold</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
    <span class="n">var_threshold</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">var_threshold</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time for feature selection: </span><span class="si">{}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X_new</span>

<span class="n">new_selected_features</span> <span class="o">=</span> <span class="n">select_features_var_threshold</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">dataframe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_selected_features</span><span class="p">)</span>
<span class="n">dataframe</span><span class="p">[</span><span class="s2">&quot;job_performance&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>
</div>
<div class="section" id="selecting-features-using-a-meta-model">
<h4>Selecting Features Using a Meta-Model</h4>
<p>The task of selecting a subset of features for final inclusion in a predictive model can also be tasked to another model (e.g., tree-based, regression or other). Similar to the low-variance feature removal procedure, a cutoff value is set and any feature that does not score above this set parameter is eliminated from the contest. Additionally, there are meta-heuristic that can be specified in order to find the optimal threshold for elimination.  By examining the the estimator's <tt class="docutils literal">coef_</tt> and <tt class="docutils literal">feature_importances_</tt> attributes, we can examine which features were selected as a result. There details are available through the documentation for the <tt class="docutils literal">sklearn.feature_selection.SelectFromModel</tt> module.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">metatransformer_fs_with_SFM</span><span class="p">(</span><span class="n">dataframe</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; uses univariate statistics such as mutual information regression to select the k best features</span>
<span class="sd">    for a regression problem &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">SelectFromModel</span>
    <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">GradientBoostingRegressor</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">xy_split</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">sfm</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># threshold=0.0001</span>
    <span class="n">sfm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">selected_features</span> <span class="o">=</span> <span class="n">sfm</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">sfm</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time for feature selection: </span><span class="si">{}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of features: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_features</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">selected_features</span>

<span class="n">new_feature_array</span> <span class="o">=</span> <span class="n">metatransformer_fs_with_SFM</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_df</span><span class="p">)</span>
<span class="n">new_df</span><span class="p">[</span><span class="s1">&#39;job_performance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>
<img alt="thresher" class="align-center" src="/assets/wheat-317021_640.jpg" style="width: 640px; height: 406px;" />
<p><a class="reference external" href="https://pixabay.com/users/Kaz-19203/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=317021">Image by Karen Arnold</a></p>
</div>
<div class="section" id="recursive-feature-elimination">
<h4>Recursive Feature Elimination</h4>
<p>The RFE approach employs a user-specified estimator to assign <em>weights</em> for the importance of features (their contribution in a subset of features) and recursively removes a number of features which fail the weight contribution requirement until a user-specified limit of inclusive features is met. Initially starting with the entire dataset, features are assigned a numeric coefficient of their importance in predicting the response variable. The features can be viewed using the <tt class="docutils literal">coef_</tt> or <tt class="docutils literal">feature_importances</tt> attribute. This method also offers the option of performing RFE with cross-validation.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fs_with_RFE</span><span class="p">(</span><span class="n">dataframe</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; recursive feature elimination best features for a regression problem &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">RFE</span>
    <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">xy_split</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>
    <span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
    <span class="n">selected_features_with_RFE</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">rfr</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsed Time for feature selection: </span><span class="si">{}</span><span class="s2">s&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">))</span>
    <span class="c1"># print(&quot;The support of each feature: {}&quot;.format(selected_features))</span>
    <span class="c1"># print(&quot;The ranking of each feature: {}&quot;.format(selected_feature_ranking))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">selected_features_with_RFE</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">selected_features_with_RFE</span>

<span class="n">selected_features_with_RFE</span>  <span class="o">=</span> <span class="n">fs_with_RFE</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">new_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">selected_features_with_RFE</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="sequential-forward-feature-selection">
<h4>Sequential Forward Feature Selection</h4>
<p><em>Sequential forward feature selection</em>  (SFFS) is a strategy to feature selection, avaiablle in the <tt class="docutils literal">mlxtend</tt> library,  which starts the predictive modeling process with a single feature at a time, and <em>sequentially</em> combining combinations of features together to select the 'best' subset of features to provide the most predictive value to your model (which you also select before hand). This SFFS also uses a meta-model to maximize the gain from any number of subsets of features in a dataset, often requiring cross-validation, and therefore can be quite time-consuming running on a single CPU.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fs_with_SFFS</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot; uses the mlxtend module to select a number of features to keep in the dataframe &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="k">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>
    <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>

    <span class="c1"># # Build RF regressor to use in feature selection</span>
    <span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">rfr</span><span class="p">,</span>
            <span class="n">k_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
            <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="n">feature_indices</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">k_feature_idx_</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">k_feature_names_</span>

    <span class="k">return</span> <span class="n">feature_indices</span><span class="p">,</span> <span class="n">feature_names</span>

<span class="c1"># We select only about a third of the features arbitrarily</span>
<span class="n">selected_feature_indices</span><span class="p">,</span> <span class="n">selected_feature_names</span> <span class="o">=</span> <span class="n">fs_with_SFFS</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="nb">round</span><span class="p">(</span><span class="mf">0.33</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">imputed_df</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span>
</pre></div>
</div>
<div class="section" id="sequential-backward-feature-selection">
<h4>Sequential Backward Feature Selection</h4>
<p><em>Sequential Backward Feature Selection</em> (SBFS), in contrast with SFFS, does the reverse in that it starts off with performing the predictive modeling task on the entire dataset first, and then removing a feature(s) each time, and selecting the best subset of features that gives the best predictive scores (either training set score or validation test scores), with cross-fold validation. Again, a fairly computationally intensive and time-consuming approach on a single CPU.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fs_with_SFFS</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot; uses the mlxtend module to select a number of features to keep in the dataframe &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="k">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>
    <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>

    <span class="c1"># # Build RF regressor to use in feature selection</span>
    <span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">rfr</span><span class="p">,</span>
            <span class="n">k_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
            <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="n">feature_indices</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">k_feature_idx_</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">k_feature_names_</span>

    <span class="k">return</span> <span class="n">feature_indices</span><span class="p">,</span> <span class="n">feature_names</span>

<span class="c1"># We select only about a third of the features arbitrarily</span>
<span class="n">selected_feature_indices</span><span class="p">,</span> <span class="n">selected_feature_names</span> <span class="o">=</span> <span class="n">fs_with_SFFS</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="nb">round</span><span class="p">(</span><span class="mf">0.33</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">imputed_df</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span>
</pre></div>
</div>
<div class="section" id="exhaustive-feature-selection">
<h4>Exhaustive Feature Selection</h4>
<p><em>Exhaustive Feature Selection</em> (EFS), as it's name suggests, algorithmically looks at every single subset combination of features</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fs_with_EFS</span><span class="p">(</span><span class="n">dataframe</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; ExhaustiveFeatureSelector &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="k">import</span> <span class="n">ExhaustiveFeatureSelector</span> <span class="k">as</span> <span class="n">EFS</span>
    <span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>

    <span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>

    <span class="n">efs1</span> <span class="o">=</span> <span class="n">EFS</span><span class="p">(</span><span class="n">rfr</span><span class="p">,</span>
            <span class="n">min_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span>
            <span class="n">print_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">efs1</span> <span class="o">=</span> <span class="n">efs1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best accuracy score: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">efs1</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best subset (indices):&#39;</span><span class="p">,</span> <span class="n">efs1</span><span class="o">.</span><span class="n">best_idx_</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best subset (corresponding names):&#39;</span><span class="p">,</span> <span class="n">efs1</span><span class="o">.</span><span class="n">best_feature_names_</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">efs1</span><span class="o">.</span><span class="n">best_idx_</span><span class="p">,</span> <span class="n">efs1</span><span class="o">.</span><span class="n">best_feature_names_</span>

<span class="n">best_feature_indices_EFS</span><span class="p">,</span> <span class="n">best_feature_names_EFS</span> <span class="o">=</span> <span class="n">fs_with_EFS</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">best_features_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">best_feature_indices_EFS</span><span class="p">,</span> <span class="p">:]</span>
</pre></div>
</div>
<div class="section" id="conclusion">
<h4>Conclusion</h4>
<p>The algorithmic approaches to feature selection/dimensionality reduction detailed in this post are by no means a complete list of what you can find out there. A brief rundown of the approaches in this post include:</p>
<ul class="simple">
<li>Exhaustive Feature Selection</li>
<li>Sequential Forward/Backward Feature Selection</li>
<li>Univariate Feature Elimination</li>
<li>Low Variance Thresholding</li>
<li>Recursive Feature Elimination</li>
<li>Feature selection with the use of meta-transformers/meta-models</li>
</ul>
<p>I encourage you to play around with any of the approaches to feature selection and go out there and discover more on your own. Be sure to also check the documentation of each one to get a better idea of the parameters, limitations and rationale for that particular approach.</p>
<p>In the <a class="reference external" href="/2019/11/sharpestminds-project-part-8.html">next post</a>, we will get into the core component of any data science project, and that is the modeling (training, testing, validating) and model selection portion. This is typically the 20% that data scientists talk about when they talk about the 80/20 split in their work. In anycase, I'll see you in the next post!</p>
</div>
</div>
</div>

            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Ever wondered what your employee performance score would be? Part 7: Feature Selection&amp;url=/2019/10/sharpestminds-project-part-7.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/2019/10/sharpestminds-project-part-7.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=/2019/10/sharpestminds-project-part-7.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="/tag/sharpestminds-project">sharpestminds-project</a><a href="/tag/data-science">data science</a><a href="/tag/projects">projects</a><a href="/tag/employee-performance">employee performance</a><a href="/tag/preprocessing">preprocessing</a><a href="/tag/feature-selection">feature selection</a><a href="/tag/feature-engineering">feature engineering</a>                </aside>

                <div class="clear"></div>

 

                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>
  
    <footer id="footer">
      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a></span>
          <span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
        </section>
      </div>
    </footer>
  </section>

  <script type="text/javascript" src="/theme/js/script.js"></script>
  
</body>
</html>
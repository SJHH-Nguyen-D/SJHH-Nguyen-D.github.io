<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Ever wondered what your employee performance score would be? Part 7: Feature Selection</title>

  <meta name="description" content="My name is Dennis Nguyen-Do. This is my personal blog." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
  <link href="" rel="canonical" />

  <!-- Feed -->

  <link href="/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="/theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->


    <link href="/2019/10/sharpestminds-project-part-7.html" rel="canonical" />

        <meta name="description" content="In this brief seventh post of the SharpestMinds project series we will go through the feature selection for our dataset and figure out...">

        <meta name="author" content="Nguyen-Do, Dennis">

        <meta name="tags" content="sharpestminds-project">
        <meta name="tags" content="data science">
        <meta name="tags" content="projects">
        <meta name="tags" content="employee performance">
        <meta name="tags" content="preprocessing">
        <meta name="tags" content="feature selection">
        <meta name="tags" content="feature engineering">

        <meta property="og:locale" content="" />
    <meta property="og:site_name" content="Denny-4/7 Data Science Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Denny-4/7 Data Science Blog" />
    <meta property="og:description" content="View the blog." />
    <meta property="og:url" content="" />
      <meta property="og:image" content="//assets/head_cover_general.jpg" />

  <meta property="og:type" content="article">
            <meta property="article:author" content="/author/nguyen-do-dennis">
  <meta property="og:url" content="/2019/10/sharpestminds-project-part-7.html">
  <meta property="og:title" content="Ever wondered what your employee performance score would be? Part 7: Feature Selection">
  <meta property="article:published_time" content="2019-10-30 18:26:00-05:00">
            <meta property="og:description" content="In this brief seventh post of the SharpestMinds project series we will go through the feature selection for our dataset and figure out...">

            <meta property="og:image" content="//assets/head_cover_general.jpg">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@Dennis00481552">
        <meta name="twitter:title" content="Ever wondered what your employee performance score would be? Part 7: Feature Selection">
        <meta name="twitter:url" content="/2019/10/sharpestminds-project-part-7.html">

            <meta name="twitter:image:src" content="//assets/head_cover_general.jpg">

            <meta name="twitter:description" content="In this brief seventh post of the SharpestMinds project series we will go through the feature selection for our dataset and figure out...">
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>

          <ul>
              <li role="presentation"><a href="/pages/about.html">About</a></li>
          <ul>
              <li role="presentation"><a href="/pages/contact.html">Contact</a></li>
              <li class="nav-archives active" role="presentation"><a href="/archives.html">Archives</a></li>
          </ul>

    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <!-- <a class="home-button" href="" title="Home"><i class="ic ic-arrow-left"></i> Home</a> -->
                <a class="home-button" href="/index.html" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Ever wondered what your employee performance score would be? Part 7: Feature Selection</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
                <a href="/author/nguyen-do-dennis">Nguyen-do, dennis</a>
            | <time datetime="Wed 30 October 2019">Wed 30 October 2019</time>
        </span>
        <!-- TODO : Modified check -->
        
            <div class="post-cover cover" style="background-image: url('/assets/head_cover_articles.jpg')">            
        
      </div>    
    </header>    

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <div class="section" id="sharpestminds-project-series-part-7-feature-selection">
<h2>SharpestMinds Project Series Part 7: Feature Selection</h2>
<p>If you are new to this post and would like some context, I'd highly suggest you read through the previous posts of this project series, as this is the seventh post of this series:</p>
<ul class="simple">
<li><a class="reference external" href="/2019/09/sharpestminds-project-part-1.html">Part 1 - Introduction</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-2.html">Part 2 - Background, Loading, EDA</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-3.html">Part 3 - More Exploratory Data Analysis</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-4.html">Part 4 - Dropping Data</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-5.html">Part 5 - Categorical Data Encoding</a></li>
<li><a class="reference external" href="/2019/10/sharpestminds-project-part-6.html">Part 6 - Imputing Missing Values</a></li>
</ul>
<p>In the previous post, we approached the problem of missing values with the use of very useful package in the PYPI library called <tt class="docutils literal">missingpy</tt> and used its <tt class="docutils literal">KNNImputer</tt> class to help better impute for the missings values in our dataset. After the imputation of missing values, which were preivously encoded with numeric values, we had to perform rounding for the categorical discrete values of our dataset to make sure that . This is the seventh post of my SharpestMinds project series go into selecting the features to be included in the modeling process of our pipelines. We use some computationally expensive, but powerful tooling provided for us through the <tt class="docutils literal">mlxtend</tt> python library from PYPI. If you want to follow along with the full codebase in the Jupyter IPython Notebook, you can do so at the <a class="reference external" href="https://github.com/SJHH-Nguyen-D/sharpestminds-project">link to the repo</a>) at the <tt class="docutils literal">sharpestminds_project.ipynb</tt> file or the <tt class="docutils literal">main.py</tt> file. Let's get into it!</p>
<div class="section" id="feature-selection">
<h3>Feature Selection</h3>
<p>In the data science pipeline, the process of 'feature selection' refers to selecting a subset of the features or attributes of our dataset to include in the final modeling process. This step could be one of the most intensive steps alongside feature engineering, and is a means to produce the most signal to our model during the training and prediction step. The reason we perform feature selection is to remove to the chaff from the wheat - that is, to remove the useless features from the truly useful ones, which would not only improve the predictive ability of our model but also improve the training times for the training of our model. This process often requires in-depth technical domain knowledge (which can be very resource expensive) or an algorithmic search strategy by which we can approach feature selection. We will go over this latter with both the regression-based approach and the tree-based approach available with the <tt class="docutils literal"><span class="pre">scikit-learn</span></tt> library. In addition, we will go over the powerful and mighty 'automatic' approach to feature selection with the <tt class="docutils literal">mlxtend</tt> library. Before we get into the code itself, I'd like to take sometime to go over some of the types of selection approaches that is typical for tabular datasets.</p>
<div class="section" id="univariate-feature-elimination">
<h4>Univariate Feature Elimination</h4>
<p>In univariate feature elimination, we use univariate statistics</p>
</div>
<div class="section" id="removing-features-with-low-variance">
<h4>Removing Features with Low Variance</h4>
<p>Numeric features that contain little to no variance do no provide more noise to signal ratio by being included in the modeling processes. Setting a cut-off value to this variance (i.e., the square of the standard deviation), allows us to eliminate noisy variables below this cut-off. By default, a variable containing only one value would be eliminated using this approach (cutoff=0.0).</p>
</div>
<div class="section" id="regression-based-feature-selection">
<h4>Regression-based feature selection</h4>
</div>
<div class="section" id="tree-based-feature-selection">
<h4>Tree-based feature selection</h4>
<img alt="thresher" class="align-center" src="/assets/wheat-317021_640.jpg" style="width: 640px; height: 406px;" />
<p><a class="reference external" href="https://pixabay.com/users/Kaz-19203/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=317021">Image by Karen Arnold</a></p>
</div>
<div class="section" id="recursive-feature-elimination">
<h4>Recursive Feature Elimination</h4>
<p>RFE...</p>
</div>
<div class="section" id="sequential-forward-feature-selection">
<h4>Sequential Forward Feature Selection</h4>
<p><em>Sequential forward feature selection</em>  (SFFS) is a strategy to feature selection which starts the predictive modeling process with a single feature at a time, and <em>sequentially</em> combining combinations of features together to select the 'best' subset of features to provide the most predictive value to your model (which you also select before hand). This SFFS process is typically also performed alongside a predictive model with k-fold cross-validation and therefore can be quite time-consuming running on a single CPU.</p>
</div>
<div class="section" id="sequential-backward-feature-selection">
<h4>Sequential Backward Feature Selection</h4>
<p><em>Sequential Backward Feature Selection</em> (SBFS), in contrast with SFFS, does the reverse in that it starts off with performing the predictive modeling task on the entire dataset first, and then removing a feature(s) each time, and selecting the best subset of features that gives the best predictive scores (either training set score or validation test scores), with cross-fold validation. Again, a fairly computationally intensive and time-consuming approach on a single CPU.</p>
</div>
<div class="section" id="exhaustive-feature-selection">
<h4>Exhaustive Feature Selection</h4>
<p><em>Exhaustive Feature Selection</em> (EFS)</p>
</div>
<div class="section" id="the-code">
<h4>The code</h4>
<p>Now, having gone through that, we are going to opt to use the SFS approach alongside a tree-based random forest regressor algorithm</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">select_n_features</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot; uses the mlxtend module to select a number of features to keep in the dataframe &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="k">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>
    <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>

    <span class="c1"># # Build RF regressor to use in feature selection</span>
    <span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">rfr</span><span class="p">,</span>
            <span class="n">k_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
            <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="n">feature_indices</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">k_feature_idx_</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">k_feature_names_</span>

    <span class="k">return</span> <span class="n">feature_indices</span><span class="p">,</span> <span class="n">feature_names</span>

<span class="c1"># We select only about a third of the features arbitrarily</span>
<span class="n">selected_feature_indices</span><span class="p">,</span> <span class="n">selected_feature_names</span> <span class="o">=</span> <span class="n">select_n_features</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="nb">round</span><span class="p">(</span><span class="mf">0.33</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">imputed_df</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span>
</pre></div>
<img alt="CoCo the cat" class="align-center" src="/assets/cocos_bizarre_adventure.jpg" style="width: 518px; height: 691px;" />
</div>
<div class="section" id="conclusion">
<h4>Conclusion</h4>
<p>This is some words about the conclusion. There! Conclusion!</p>
</div>
</div>
</div>

            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Ever wondered what your employee performance score would be? Part 7: Feature Selection&amp;url=/2019/10/sharpestminds-project-part-7.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/2019/10/sharpestminds-project-part-7.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=/2019/10/sharpestminds-project-part-7.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="/tag/sharpestminds-project">sharpestminds-project</a><a href="/tag/data-science">data science</a><a href="/tag/projects">projects</a><a href="/tag/employee-performance">employee performance</a><a href="/tag/preprocessing">preprocessing</a><a href="/tag/feature-selection">feature selection</a><a href="/tag/feature-engineering">feature engineering</a>                </aside>

                <div class="clear"></div>

 

                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>
  
    <footer id="footer">
      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a></span>
          <span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
        </section>
      </div>
    </footer>
  </section>

  <script type="text/javascript" src="/theme/js/script.js"></script>
  
</body>
</html>